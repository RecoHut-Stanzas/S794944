{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Executive summary\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| Problem | GCNs suffer from training difficulty due to non-linear activations, and over-smoothing problem. |\n",
        "| Hypothesis | removing non-linearities would enhance recommendation performance.  |\n",
        "| Solution | Linear model with residual network structure |\n",
        "| Dataset | Gowalla, Amazon-books |\n",
        "| Preprocessing | we remove users (items) that have less than 10 interaction records. After that, we randomly select 80% of the records for training, 10% for validation and the remaining 10% for test. |\n",
        "| Metrics | HR, NDCG |\n",
        "| Hyperparams | There are two important parameters: the dimension D of the user and item embedding matrix E, and the regularization parameter Î» in the objective function. The embedding size is fixed to 64. We try the regularization parameter Î» in the range [0.0001, 0.001, 0.01, 0.1], and find Î» = 0.01 reaches the best performance. |\n",
        "| Models | LR-GCCF |\n",
        "| Cluster | PyTorch with GPU |\n",
        "\n",
        "## Model\n",
        "\n",
        "![Figure: The overall architecture of LR-GCCF](https://github.com/RecoHut-Stanzas/S794944/raw/main/images/Overall_framework.jpg)\n",
        "\n",
        "Figure: The overall architecture of LR-GCCF\n",
        "\n",
        "## Graph Convolutional Networks (GCN)\n",
        "\n",
        "GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. GCN-based methods can effectively learn the behavioral patterns between users and items by directly capturing the collaborative signals inherent in the user-item interactions. Typical GCN-based methods include GC-MC, PinSage, and NGCF.\n",
        "\n",
        "GCN-based methods model a set of user-item interactions as a user-item bipartite graph and then perform the following three steps:\n",
        "\n",
        "1. **Initialization** - They randomly set the initial embedding $ğ’†^0$ of all user ğ‘¢ and item ğ‘£, denoted as $ğ’†_u^0,ğ’†_ğ‘£^0 \\in \\mathbb{R}^ğ·$, where ğ· denotes the embedding size.\n",
        "2. **Propagation** - First of all, this propagation step is iterated ğ¾ times, i.e., ğ¾ layers of embedding propagation. Given the ğ¾ layers, the embedding of a user node ğ‘¢ (resp. an item node ğ‘£) in ğ‘–-th layer is updated based on the embeddings of ğ‘¢â€™s (resp. ğ‘£â€™s) neighbors $ğ‘_ğ‘¢$ (resp. $ğ‘_ğ‘£$) in (ğ‘– âˆ’ 1)-th layer as $e_u^i = \\sigma(\\sum_{v \\in N_u}e_v^{i-1}W_i)$, and $e_v^i = \\sigma(\\sum_{u \\in N_v}e_u^{i-1}W_i)$, where ğœ denotes a non-linear activation function, e.g., ReLU, and $ğ‘¾_ğ‘– \\in \\mathbb{R}^{ğ·Ã—ğ·}$ is a trainable transformation matrix. There exist some other variations: i) including the self-embeddings, i.e., $ğ‘_ğ‘¢ = ğ‘_ğ‘¢ âˆª \\{ğ‘¢\\}$ and $ğ‘_ğ‘£ = ğ‘_ğ‘£ âˆª \\{ğ‘£\\}$, ii) removing the transformation matrix, and iii) removing the non-linear activation, which is in particular called as linear propagation.\n",
        "3. **Prediction** - The preference of user ğ‘¢ to item ğ‘£ is typically predicted using the dot product between the user ğ‘¢â€™s and item ğ‘£â€™s embeddings in the last layer ğ¾, i.e., $ğ’†_ğ‘¢^ğ¾$ and $ğ’†_ğ‘£^ğ¾$, as $\\hat{r}_{u,v}=e_u^K \\odot e_v^K$.\n",
        "\n",
        "### Message Passing Procedure\n",
        "\n",
        "[https://youtu.be/ijmxpItkRjc](https://youtu.be/ijmxpItkRjc)\n",
        "\n",
        "The message passing layer with self-loops is defined as follows:\n",
        "\n",
        "$$E^{(l+1)} = \\sigma(\\hat{D}^{-1/2} \\hat{A}\\hat{D}^{-1/2}E^{(l)}W^{(l)})$$\n",
        "\n",
        "where,\n",
        "\n",
        "- $\\hat{A} = A + I$, and $\\hat{D} = D + I$\n",
        "- $ğ´$, $ğ·$, $ğ¼$ are the adjacency matrix, the diagonal node degree matrix, and the identity matrix, respectively\n",
        "- $ğ¼$ is used to integrate self-loop connections on nodes\n",
        "- $ğ¸^{(ğ‘™)}$ and $W^{(ğ‘™)}$ denote the representation matrix and the weight matrix for the $ğ‘™$-th layer\n",
        "- $\\sigma(Â·)$ is a non-linear activation function (e.g., ReLU).\n",
        "\n",
        "LightGCN is the simplified GCN model that removes feature transformations (i.e., $ğ‘Š^{(ğ‘™)}$) and non-linear activations (i.e., ğœ). Its message passing layer can thus be expressed as follows:\n",
        "\n",
        "$$E^{(l+1)} = (\\hat{D}^{-1/2} \\hat{A}\\hat{D}^{-1/2}E^{(l)})$$\n",
        "\n",
        "Given self-loop connections, we can rewrite the message passing operations for user ğ‘¢ and item ğ‘– as follows:\n",
        "\n",
        "$$e_u^{(l+1)} = \\dfrac{1}{d_u + 1} e_u^{(l)} + \\sum_{k \\in \\mathcal{N}(u)} \\dfrac{1}{\\sqrt{d_u+1}\\sqrt{d_k+1}} e_k^{(l)}$$\n",
        "\n",
        "$$e_i^{(l+1)} = \\dfrac{1}{d_i + 1} e_i^{(l)} + \\sum_{v \\in \\mathcal{N}(i)} \\dfrac{1}{\\sqrt{d_i+1}\\sqrt{d_i+1}} e_v^{(l)}$$\n",
        "\n",
        "where,\n",
        "\n",
        "- ğ‘¢ and ğ‘£ denote users while ğ‘– and ğ‘˜ denote items\n",
        "- $e_u^{(l)}$ and $e_i^{(l)}$ denote the embeddings of user ğ‘¢ and item ğ‘– at layer ğ‘™\n",
        "- $\\mathcal{N}(ğ‘¢)$ and $\\mathcal{N}(ğ‘–)$ represent their neighbor node sets, respectively\n",
        "- $ğ‘‘_ğ‘¢$ denotes the original degree of the node ğ‘¢.\n",
        "\n",
        "LightGCN takes the dot product of the two embedding as the final logit to capture the preference of user ğ‘¢ on item ğ‘–. Thus we obtain:\n",
        "\n",
        "$$e_u^{(l+1)} \\cdot e_i^{(l+1)} = \\alpha_{ui}(e_u^{(l)} \\cdot e_i^{(l)}) + \\sum_{k \\in \\mathcal{N}(u)}\\alpha_{ik}(e_i^{(l)} \\cdot e_k^{(l)}) + \\sum_{v \\in \\mathcal{N}(i)}\\alpha_{uv}(e_u^{(l)} \\cdot e_v^{(l)}) + \\sum_{k \\in \\mathcal{N}(u)}\\sum_{v \\in \\mathcal{N}(i)}\\alpha_{kv}(e_k^{(l)} \\cdot e_v^{(l)})$$\n",
        "\n",
        "Therefore, we can observe that multiple different types of collaborative signals, including user-item relationships (ğ‘¢-ğ‘– and ğ‘˜-ğ‘£), item-item relationships (ğ‘˜-ğ‘–), and user-user relationships (ğ‘¢-ğ‘£), are captured when training GCN-based models with message passing layers. This also reveals why GCN-based models are effective for CF.\n",
        "\n",
        "## Over-smoothing Problem in GCNs\n",
        "\n",
        "Over-smoothing is caused as they use only the embeddings updated through the last layer in the prediction layer. Specifically, as the number of layers increases, the embedding of a node will be influenced more from its neighborsâ€™ embeddings. As a result, the embedding of a node in the last layer becomes similar to the embeddings of many directly/indirectly connected nodes. This phenomenon prevents most of the existing GCN-based methods from effectively utilizing the information of high-order neighborhood. Empirically, this is also shown by the fact that most of non-linear GCN-based methods show better performance when using only a few layers instead of deep networks. The common solution is **Residual Prediction** - Utilize the embeddings from all layers for prediction. After that, perform residual prediction, which predict each userâ€™s preference to each item with the multiple embeddings from the multiple layers.\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "### LR-GCCF Model Training on Gowalla Dataset in PyTorch\n",
        "\n",
        "[Link to notebook â†’](https://github.com/RecoHut-Stanzas/S794944/blob/main/nbs/P174968_LR_GCCF_on_Gowalla.ipynb)\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S794944/raw/main/images/process_flow.svg](https://github.com/RecoHut-Stanzas/S794944/raw/main/images/process_flow.svg)\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://github.com/RecoHut-Stanzas/S794944](https://github.com/RecoHut-Stanzas/S794944)\n",
        "2. [https://arxiv.org/abs/2001.10167](https://arxiv.org/abs/2001.10167)\n",
        "3. [https://github.com/newlei/LR-GCCF](https://github.com/newlei/LR-GCCF)"
      ],
      "metadata": {
        "id": "lIYdn1woOS1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gKdyKvStgxe9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}